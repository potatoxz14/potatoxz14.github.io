---
title: ğŸ‰ AI | å­¦ä¹ ç¬”è®°
summary: AIå­¦ä¹ ç¬”è®°
date: 2025-08-29

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com)'

authors:
  - admin

tags:
  - AI
---
**Activation Sparsity = å¾ˆå¤šç¥ç»å…ƒåœ¨å¤„ç†æŸä¸ªè¾“å…¥æ—¶â€œä¿æŒæ²‰é»˜â€ï¼Œè¾“å‡ºä¸º 0 æˆ–æ¥è¿‘ 0 çš„æ¯”ä¾‹å¾ˆé«˜**ã€‚

**Skewness**ï¼ˆååº¦ï¼‰æ˜¯è¡¡é‡æ•°æ®åˆ†å¸ƒå¯¹ç§°æ€§çš„ä¸€ç§æŒ‡æ ‡ï¼Œé€šå¸¸æ˜¯æŒ‡ï¼š

- **æŸäº› neuron æ¿€æ´»å€¼çš„åæ–œç¨‹åº¦**
- **token å‡ºç°é¢‘ç‡åˆ†å¸ƒçš„ååº¦**
- **layer è¾“å‡ºåˆ†å¸ƒæ˜¯å¦å¯¹ç§°**

å“ªäº› neuron çš„æ¿€æ´»åˆ†å¸ƒâ€œååº¦å¾ˆå¤§â€ â†’ è¯´æ˜å®ƒä¸æ˜¯ç»å¸¸æ¿€æ´»ï¼ˆå†· neuronï¼‰ï¼

discrepancyï¼šå·®å¼‚



Transformer Layer
â”œâ”€â”€ Self-Attentionï¼ˆå« Multi-Headï¼‰
â””â”€â”€ MLP

Attention(Q, K, V) = softmax(QK^T / sqrt(d)) * V

#### ğŸŒˆ ä»€ä¹ˆæ˜¯å¤šå¤´ï¼Ÿ

> å°±æ˜¯å¹¶è¡Œåœ°åš **å¤šä¸ªä¸åŒçš„ self-attention**ï¼Œæ¯ä¸ªå¤´è´Ÿè´£â€œçœ‹ä¸åŒè§’åº¦â€ã€‚

#### ğŸŒŸ ä¸ºä»€ä¹ˆå¤šä¸ªå¤´æœ‰ç”¨ï¼Ÿ

- å‡è®¾ä½ æœ‰ 8 ä¸ªå¤´ï¼š
  - ç¬¬ä¸€ä¸ªå¤´å…³æ³¨è¯­æ³•ç»“æ„
  - ç¬¬äºŒä¸ªå¤´å…³æ³¨ä¸»è°“å…³ç³»
  - ç¬¬ä¸‰ä¸ªå¤´å…³æ³¨æƒ…æ„Ÿè¯
  - ç¬¬å››ä¸ªå¤´ä¸“é—¨å¤„ç†è¿œè·ç¦»ä¾èµ–
  - â€¦
- å¤šä¸ªå¤´å¯ä»¥å¹¶è¡Œä»ä¸åŒâ€œå­ç©ºé—´â€å­¦ä¹ ç‰¹å¾ä¿¡æ¯

 Neuron 

## ğŸ§  åœ¨ç¥ç»ç½‘ç»œé‡Œï¼ŒNeuron åšäº†ä»€ä¹ˆï¼Ÿ

æ¯ä¸ªç¥ç»å…ƒä¼šï¼š

1. æ¥æ”¶ä¸€äº›è¾“å…¥ï¼ˆæ¥è‡ªå‰ä¸€å±‚çš„ç¥ç»å…ƒï¼‰
2. æŠŠè¿™äº›è¾“å…¥ä¹˜ä¸Šè‡ªå·±çš„â€œæƒé‡â€ + åŠ ä¸Šä¸€ä¸ªâ€œåç½®â€ï¼ˆbiasï¼‰
3. é€šè¿‡ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆæ¯”å¦‚ ReLUã€GELUï¼‰å¤„ç†ä¸€ä¸‹
4. è¾“å‡ºç»“æœï¼Œä¼ ç»™ä¸‹ä¸€å±‚ç¥ç»å…ƒ

Self-Attention
â”œâ”€â”€ Multi-Head Attention
â”‚   â”œâ”€â”€ Head 1: Attention(Q1, K1, V1)
â”‚   â”œâ”€â”€ Head 2: Attention(Q2, K2, V2)
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ Head N
â”‚   â””â”€â”€ Concat â†’ Linear Projection
â””â”€â”€ æ®‹å·®è¿æ¥ + LayerNorm

## ğŸ§  Neuron åœ¨ Transformer / LLM ä¸­çš„è§’è‰²ï¼Ÿ

è™½ç„¶ Transformer æ›´åå‘ Attention ç»“æ„ï¼Œä½† **MLP éƒ¨åˆ†å°±æ˜¯ç”± neuron æ„æˆçš„å‰é¦ˆç¥ç»ç½‘ç»œ**ã€‚

åœ¨ GPT ç­‰ LLM ä¸­ï¼š

- æ¯ä¸ª Transformer Layer çš„ MLP éƒ¨åˆ†é€šå¸¸æœ‰ï¼š
  - **ä¸€å±‚å‡ç»´**ï¼ˆæ¯”å¦‚ä» 4096 -> 16384ï¼‰ï¼ˆåŒ…å«å¤šä¸ªneuronï¼‰
  - **æ¿€æ´»å‡½æ•°**
  - **ä¸€å±‚é™ç»´**ï¼ˆå†ä» 16384 -> 4096ï¼‰ï¼ˆåŒ…å«å¤šä¸ªneuronï¼‰

> æ‰€ä»¥ä¸€ä¸ª LLM æ¨¡å‹é‡Œï¼Œå…¶å®åŒ…å«äº† **æ•°åäº¿ç”šè‡³ä¸Šä¸‡äº¿ä¸ª neuron**ï¼

å‰æ–‡ä¸­çš„å®ƒæŠŠè¾“å…¥çš„åºåˆ—è½¬æˆ Queryï¼ˆQï¼‰ã€Keyï¼ˆKï¼‰ã€Valueï¼ˆVï¼‰ä¸‰ä¸ªçŸ©é˜µ

## ğŸ§  ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼šä¸€å¥è¯

```
text


å¤åˆ¶ç¼–è¾‘
"Tom loves pizza"
```

å‡è®¾è¾“å…¥è¢«è¡¨ç¤ºæˆäº†ä¸‰ä¸ª embeddingï¼Œè¿™é‡Œçš„embeddingæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å›ºå®šä¸‹æ¥çš„ï¼ˆç›¸å¯¹ç¨³å®šï¼‰ã€‚ï¼ˆæ¯ä¸ªè¯å˜æˆä¸€ä¸ªå‘é‡ï¼‰ï¼š

```
Tom    â†’ xâ‚ = [0.1, 0.3, 0.5]
loves  â†’ xâ‚‚ = [0.2, 0.1, 0.7]
pizza  â†’ xâ‚ƒ = [0.5, 0.4, 0.2]
```

## ğŸ”§ æ€ä¹ˆå˜æˆ Qã€Kã€Vï¼Ÿ

å¾ˆç®€å•ï¼š

> å°±æ˜¯å¯¹æ¯ä¸ªè¾“å…¥å‘é‡ **ä¹˜ä»¥ä¸åŒçš„æƒé‡çŸ©é˜µ**ï¼ˆ**æ¨¡å‹çš„æ ¸å¿ƒå‚æ•°**ï¼Œæ•°é‡å·¨å¤§ï¼ˆGPT-3 æœ‰ 1750 äº¿ä¸ªå‚æ•°ï¼ï¼‰ï¼‰ï¼

```
Q = X @ W_q
K = X @ W_k
V = X @ W_v
```

å…¶ä¸­ï¼š

- `X` æ˜¯è¾“å…¥çš„ embeddingï¼ˆå½¢çŠ¶æ˜¯ `[seq_len, d_model]`ï¼‰
- `W_q`, `W_k`, `W_v` æ˜¯æ¨¡å‹å­¦åˆ°çš„æƒé‡ï¼ˆæ¯”å¦‚ `[d_model, d_k]`ï¼‰

## âœ… æ€»ç»“ä¸€å¥è¯ï¼š

| é—®é¢˜                   | ç­”æ¡ˆ                                |
| ---------------------- | ----------------------------------- |
| Tom ä¼šå˜æˆå‘é‡å—ï¼Ÿ     | âœ… æ˜¯çš„ï¼Œé€šè¿‡ embedding              |
| æ˜¯å›ºå®šçš„å—ï¼Ÿ           | æ¨ç†æ—¶å›ºå®šï¼Œè®­ç»ƒæ—¶å¯æ›´æ–°            |
| embedding æ˜¯æ€ä¹ˆç®—çš„ï¼Ÿ | æŸ¥ embedding è¡¨ï¼Œå‘é‡ç”±æ¨¡å‹è®­ç»ƒå¾—å‡º |
| embedding æœ‰è¯­ä¹‰å—ï¼Ÿ   | âœ… æœ‰ï¼Œæ¯”å¦‚â€œè¿‘ä¹‰è¯â€ä¼šé å¾—è¿‘          |

## MLP

**MLPï¼ˆMulti-Layer Perceptronï¼Œå¤šå±‚æ„ŸçŸ¥æœºï¼‰å°±æ˜¯ä¸€æ®µæ™®é€šçš„â€œå‰é¦ˆç¥ç»ç½‘ç»œâ€**ï¼Œå®ƒåœ¨ Transformer çš„æ¯ä¸€å±‚é‡Œéƒ½è´Ÿè´£éçº¿æ€§å¤„ç†ã€ä¿¡æ¯æ··åˆã€‚

åœ¨ Transformer ä¸­ï¼ŒMLP é€šå¸¸å°±ä¸¤å±‚çº¿æ€§å˜æ¢ï¼Œä¸­é—´åŠ ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆéçº¿æ€§ï¼‰ï¼š

### ğŸ§  é€šä¿—ç‚¹è§£é‡Šï¼š

1. è¾“å…¥å‘é‡ xï¼ˆæ¯”å¦‚ Attention è¾“å‡ºï¼‰
2. å…ˆé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ `W1` â†’ æŠŠä¿¡æ¯â€œå‡ç»´â€
3. ç»è¿‡éçº¿æ€§å‡½æ•° `ReLU`ï¼ˆåŠ å…¥â€œåˆ¤æ–­åŠ›â€ï¼‰
4. å†é€šè¿‡å¦ä¸€ä¸ªçº¿æ€§å±‚ `W2` â†’ â€œé™ç»´â€å›æ¥

## ğŸ” ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼š

å‡è®¾ä½ è¾“å…¥äº†ä¸€ä¸ªç»´åº¦ä¸º 512 çš„å‘é‡ï¼ˆæ¨¡å‹ä¸­çš„æŸä¸ª token è¡¨ç¤ºï¼‰ï¼š

1. **ç¬¬ä¸€å±‚ï¼ˆå‡ç»´ï¼‰**ï¼š
   - è¾“å…¥ 512 â†’ è¾“å‡º 2048ï¼ˆæ¯”å¦‚æ‰©å±•åˆ°æ›´å¤§çš„ç‰¹å¾ç©ºé—´ï¼‰
2. **ReLU éçº¿æ€§**ï¼š
   - æŠŠè´Ÿå€¼æˆªæ–­æˆ 0ï¼Œå¼•å…¥â€œåˆ¤æ–­åŠ›â€å’Œâ€œéçº¿æ€§åŒºåˆ†èƒ½åŠ›â€
3. **ç¬¬äºŒå±‚ï¼ˆé™ç»´ï¼‰**ï¼š
   - è¾“å…¥ 2048 â†’ è¾“å‡º 512ï¼ˆå˜å›åŸæ¥çš„ç»´åº¦ï¼‰

## ğŸ§ª ä¸ºä»€ä¹ˆéœ€è¦ MLPï¼Ÿ

å› ä¸ºä»…é  Attentionï¼Œæ¨¡å‹åªæ˜¯æŠŠ token ä¿¡æ¯åœ¨å½¼æ­¤ä¹‹é—´â€œè·¯ç”±â€äº†ã€‚

> MLP çš„ä½œç”¨æ˜¯ï¼š**æ¯ä¸ª token å†…éƒ¨åšæ›´å¤æ‚çš„ã€éçº¿æ€§çš„å˜æ¢ã€‚**

ä½ å¯ä»¥ç†è§£ä¸ºï¼š

> Attention æ˜¯â€œæˆ‘å’Œåˆ«äººæ€ä¹ˆäº’åŠ¨â€ï¼ŒMLP æ˜¯â€œæˆ‘è‡ªå·±å†…éƒ¨åšä»€ä¹ˆå†³å®šâ€ã€‚



## predictors

**MLP predictors** ä¸­çš„ **predictors** åˆ°åº•æŒ‡ä»€ä¹ˆ ğŸ‘‡

## âœ… ä¸€å¥è¯è§£é‡Šï¼š

> åœ¨â€œMLP predictorsâ€è¿™ä¸ªçŸ­è¯­ä¸­ï¼Œ**predictors å°±æ˜¯ç”¨æ¥é¢„æµ‹ç»“æœçš„ MLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰æ¨¡å—**ã€‚

å®ƒæ˜¯ç¥ç»ç½‘ç»œä¸­ä¸“é—¨**æ‰§è¡Œé¢„æµ‹ä»»åŠ¡**çš„éƒ¨åˆ†ã€‚

ä»–ä»¬ä¸ºæ¯ä¸€å±‚ Transformer å±‚éƒ½è®¾è®¡äº†ä¸€ä¸ª **ä¸“å±çš„ predictorï¼ˆMLP æ¨¡å‹ï¼‰**ï¼Œè¿™ä¸ª predictor çš„å¤§å°ï¼ˆéšè—å±‚ç»´åº¦ï¼‰ä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯æ ¹æ®è¯¥å±‚çš„æ¿€æ´»ç‰¹å¾**åŠ¨æ€è°ƒæ•´çš„**ã€‚

## ğŸ“Œ ç¬¬äºŒæ®µï¼šå»ºç«‹åˆå§‹è§„æ¨¡

> The process begins by establishing a baseline model size based on the layerâ€™s sparsity profile.

### âœ… å«ä¹‰ï¼š

- æ¯å±‚ Transformer çš„ **ç¨€ç–æ€§ä¸åŒ**ï¼ˆå³æ¿€æ´»ä¸º 0 çš„ neuron æ¯”ä¾‹ä¸åŒï¼‰ã€‚
- æ ¹æ®è¿™ä¸ª sparsityï¼ˆæ¯”å¦‚ 90% neuron éƒ½ä¸åŠ¨ï¼‰ï¼Œå…ˆå®šä¸€ä¸ªåˆå§‹çš„ predictor å¤§å°ã€‚

## ğŸ” ç¬¬ä¸‰æ®µï¼šè¿­ä»£è°ƒæ•´éšè—å±‚

> the model size is iteratively adjusted, considering the internal activation skewness to maintain accuracy.

### âœ… å«ä¹‰ï¼š

- ç„¶åé€šè¿‡è¿­ä»£æ–¹å¼è°ƒæ•´è¿™ä¸ª predictor çš„â€œéšè—å±‚ç»´åº¦â€ï¼›
- ä¾æ®æ˜¯è¯¥å±‚ **activation skewness**ï¼ˆæ¿€æ´»å€¼æ˜¯å¦é›†ä¸­äºæŸäº› neuronï¼Œååº¦é«˜è¯´æ˜åˆ†å¸ƒä¸å‡ï¼‰ï¼›
- ç›®çš„æ˜¯ï¼š**ä¿è¯å‡†ç¡®ç‡ï¼ˆç²¾åº¦ï¼‰ä¸ä¸‹é™ã€‚**

å»ºç«‹äº†ä¸¤å¼ ç¥ç»å…ƒçš„è¡¨ã€‚ä¸€ä¸ªæ”¾åœ¨cpuä¸€ä¸ªæ”¾åœ¨GPUã€‚è¿™ä¸ªè¡¨ correlate each neuron to its original position in the matrix.ï¼ˆæ¨ç†æ—¶ï¼ŒPowerInfer æŒ‰è¿™ä¸ªç­–ç•¥æŠŠæƒé‡çŸ©é˜µ **åˆ†å‰²å¹¶åŠ è½½**åˆ°ä¸åŒè®¾å¤‡å†…å­˜ä¸­ï¼ˆGPU vs CPUï¼‰ï¼‰

### ğŸ§± 2. æ¯ä¸€å±‚çš„ç¥ç»å…ƒæ˜¯æ€ä¹ˆåˆ†çš„ï¼Ÿ

> For each layer, which may consist of multiple weight matrices, PowerInfer assigns each neuron to either the GPU or CPU based on whether the neuron is hot-activated.

âœ… å«ä¹‰ï¼š

- æ¯ä¸€å±‚ï¼ˆæ¯”å¦‚ MLP å±‚ï¼‰éƒ½æœ‰å¤šä¸ªæƒé‡çŸ©é˜µï¼ˆæ¯”å¦‚ FC1ã€FC2ï¼‰
- æ¯ä¸ª neuronï¼ˆå³æ¯ä¸ªè¾“å‡ºç»´åº¦ï¼‰éƒ½å¯ä»¥**å•ç‹¬è¢«åˆ†é…åˆ° GPU æˆ– CPU**
- åˆ†é…æ ‡å‡†ï¼šå®ƒæ˜¯ä¸æ˜¯é«˜é¢‘æ¿€æ´»çš„ neuronï¼ˆhot neuronï¼‰

âœ–ï¸ğŸŸ° 5. å®é™…çŸ©é˜µä¹˜æ³•æ€ä¹ˆåšï¼Ÿ

During the process of multiplying with an input tensor, each neuron interacts with its corresponding tensor value, guided by the mappings in the neuron tables.
åœ¨å®é™…ä¸è¾“å…¥å¼ é‡è¿›è¡Œå¤„ç†çš„æ—¶å€™ï¼Œæ¯ä¸€ä¸ªç¥ç»å…ƒéƒ½ä¼šåœ¨neuron tablesçš„æŒ‡å¼•ä¸‹ä¸ç›¸åº”çš„tensor valueç›¸è¿ç®—

##  GPU-CPU Hybrid Execution

PowerInferä¼šæ„å»ºä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½ä»£è¡¨ä¸€ä¸ªè®¡ç®—çš„LLM inference operatorã€‚å¹¶ä¸”éƒ½ä¼šä¿å­˜åœ¨CPUçš„å†…å­˜ä¸­ã€‚æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½ä¼šè¢«tagged with its prerequisite operators.

åœ¨æ¨ç†é˜¶æ®µï¼Œä»å…¨å±€çš„é˜Ÿåˆ—é‡Œé¢pull operators from the global queue, check dependencies, and assign them to the appropriate processing unit.

### ğŸ§µ çº¿ç¨‹è°ƒåº¦å™¨ï¼ˆexecutorsï¼‰

> ... two types of executors, **pthreads** created by the host OS, manage calculations on both CPU and GPU.

- ç³»ç»Ÿç”¨ POSIX çº¿ç¨‹ï¼ˆpthreadï¼‰åˆ›å»ºä¸¤ä¸ªè°ƒåº¦å™¨ï¼š
  - ä¸€ä¸ªè°ƒåº¦ GPU ä¸Šçš„ä»»åŠ¡
  - ä¸€ä¸ªè°ƒåº¦ CPU ä¸Šçš„ä»»åŠ¡

### ğŸ”„ æ‰§è¡Œå™¨å¦‚ä½•è°ƒåº¦ä»»åŠ¡ï¼Ÿ

> They pull operators from the global queue, check dependencies, and assign them to the appropriate processing unit.

- æ¯ä¸ªæ‰§è¡Œå™¨ä¸æ–­ä»é˜Ÿåˆ—é‡Œæ‹‰å– operator
- æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³ï¼ˆå‰ç½®ä»»åŠ¡æ˜¯å¦éƒ½å·²å®Œæˆï¼‰
- ç„¶åæŠŠä»»åŠ¡åˆ†æ´¾ç»™ CPU æˆ– GPU å¯¹åº”çš„æ‰§è¡Œå‡½æ•°

### ğŸ’¡ GPU ä¸ CPU æ‰§è¡Œç»†èŠ‚

> The GPU and CPU use their **neuron-aware operators**, with the GPU executor launching GPU operators using APIs like **cudaLaunchKernel**, and the CPU executor coordinating unoccupied CPU cores...

- æ¯ä¸ªä»»åŠ¡æ˜¯â€œneuron-awareâ€çš„ï¼Œå³çŸ¥é“è‡ªå·±è´Ÿè´£çš„æ˜¯å“ªéƒ¨åˆ† neuronï¼ˆä¹‹å‰æˆ‘ä»¬è®²è¿‡çš„çƒ­ neuron/cold neuronï¼‰
- GPU ç«¯é€šè¿‡ `cudaLaunchKernel` è¿™æ ·çš„ API è°ƒç”¨ GPU è¿ç®—
- CPU ç«¯åˆ†é…å½“å‰ç©ºé—²çš„æ ¸ï¼ˆæ ¸å¤šä»»åŠ¡è°ƒåº¦ï¼‰
- å¦‚æœæŸä¸ª operator åœ¨ CPU ä¸Šæ‰§è¡Œï¼Œå®ƒçš„ä¾èµ–ï¼ˆparent nodeï¼‰å´åœ¨ GPU ä¸Šæ‰§è¡Œ â†’ é‚£å¿…é¡»ç­‰ GPU å®Œæˆ
- æ‰€ä»¥ä½¿ç”¨ **barrierï¼ˆå±éšœæœºåˆ¶ï¼‰** æ¥ç¡®ä¿ä¾èµ–å®Œæˆæ‰å¼€å§‹

PowerInfer å¼•å…¥äº†neuron-awareçš„operatorï¼Œè¿™ä¸œè¥¿ç›´æ¥è®¡ç®—activated neuronsä»¥åŠtheir weights without the need for runtime conversion to dense format. These operators differ from traditional ones as they focus on individual row/column vectors within a matrix rather than the entire matrix.

## ğŸ”§ Neuron-aware Operators on **GPU**

### âœ… ä¸ºä»€ä¹ˆè¦ç”¨ vector-vector è€Œä¸æ˜¯ matrix-vectorï¼Ÿ

> ... **vector-vector calculations** being less efficient than matrix-vector calculations on GPU, neuron-aware operators based on vector-vector computation are advantageous when the **batch size is small**.

**è§£é‡Šï¼š**

- **ä¼ ç»Ÿ LLM ç”¨çš„æ˜¯çŸ©é˜µä¹˜æ³•**ï¼ˆæ¯”å¦‚ weight çŸ©é˜µ Ã— è¾“å…¥å‘é‡ï¼‰
- ä½†å½“æ¿€æ´»ç¨€ç–æ€§é«˜ã€batch size å°æ—¶ï¼Œå¤§é‡ neuron æ˜¯â€œç©ºè®¡ç®—â€
- æ­¤æ—¶ç”¨ **é€ neuron çš„ vector-vector ç‚¹ä¹˜**ï¼Œå¯ä»¥è·³è¿‡æœªæ¿€æ´» neuronï¼Œ**æ•´ä½“æ•ˆç‡åè€Œæ›´é«˜**
- ä¸åƒä¸€äº›ç¨€ç–å·¥å…·éœ€è¦å…ˆæŠŠç¨€ç–çŸ©é˜µè½¬æˆå¯†é›†æ ¼å¼å†è®¡ç®—ï¼ˆå¦‚ CSR â†’ denseï¼‰
- è¿™ç§è®¾è®¡ä¸­ **å®Œå…¨è·³è¿‡æœªæ¿€æ´» neuron**ï¼Œä¹Ÿä¸éœ€è¦ä»»ä½•æ ¼å¼è½¬æ¢æˆ–æ•°æ®å¡«å……
- **æ¯ä¸ª GPU thread block è¢«åˆ†é…ä¸€æ‰¹ neuron**
- å®ƒè´Ÿè´£æ£€æŸ¥è¿™äº› neuron æœ‰æ²¡æœ‰è¢«æ¿€æ´»ï¼Œå¦‚æœæ¿€æ´»å°±è®¡ç®—
- æ¯ä¸ª block éƒ½æ˜¯â€œç‹¬ç«‹å¤„ç†â€çš„ï¼Œæ‰€ä»¥ **æ— éœ€çº¿ç¨‹é—´åŒæ­¥** â†’ é¿å…äº† warp divergence / lock ç­‰æ€§èƒ½é—®é¢˜

ğŸ§® Neuron-aware Operators on **CPU**

- ç›¸æ¯” GPU çš„å‡ åƒçº¿ç¨‹ï¼ŒCPU å¹¶å‘æ ¸å°‘ã€SIMD ååä½
- æ‰€ä»¥ CPU æ›´éœ€è¦é¿å…â€œæ— ç”¨è®¡ç®—â€ï¼Œç²¾ç»†æ§åˆ¶ **è°è¯¥ç®—ã€è°ä¸è¯¥ç®—**
- **å¤šä¸ª CPU æ ¸å¿ƒ**å¹¶è¡Œå¤„ç† neuron çš„ batch
- æ¯ä¸ªæ ¸å¿ƒåªç®—è¢«æ¿€æ´»çš„ neuron
- ç”¨ **AVX2 ç­‰ SIMD æŒ‡ä»¤é›†** åŠ é€Ÿâ€œé€ neuron çš„ç‚¹ä¹˜â€æ“ä½œï¼ˆå³ä¸€æ¬¡ç®—å¤šä¸ª floatï¼‰

## Neuron Placement Policy

###  Communication Constraint 

GPUä¸ŠåŠ ä¸Šä¼ è¾“æ—¶é—´å°äºåœ¨CPUä¸Šè¿è¡Œçš„æ—¶é—´ï¼Œä»¥layerä¸ºå•ä½ã€‚

![image-20250421173329718](D:\ç ”ç©¶ç”Ÿæ•°æ®\ç ”ç©¶æ•´ç†\è®ºæ–‡\AI agent\å›¾ç‰‡\image-20250421173329718.png)

### Memory Constraint

when allocating neurons of a layer to the GPU, it either assigns at least the minimum number of neurons specified in Inequality 4 to offset communication costs or opts not to allocate any neurons from that layer to the GPU. Specifically, the number of neurons for layer ğ‘™ on the GPU must either exceed ğ¶ğ‘™ or be equal to zero.